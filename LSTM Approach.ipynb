{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM , SimpleRNN\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Flatten, Reshape\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utilities.config_handler import get_config\n",
    "from utilities.learning import  get_clipped_loss, split_train_validation, \\\n",
    "    train_model, predict_error_vectors, reshape_errors, train_gmm\n",
    "from utilities.preprocessing import series_to_supervised, trim_by_seq_length, reshape_to_seq, \\\n",
    "    get_X_and_Y_columns,persist_object,load_object, load_test_data, load_train_data, scale_train_vectors, \\\n",
    "    scale_test_vectors\n",
    "from utilities.detection import compute_emd_split_samples,compute_emd_distributions, detect_anomalies_median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.argv = '-m train -n iq_data/gps_new/norm --weights-save-path model/rnn_model.hdf5'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.prog = 'Spectrum Anomaly Detection'\n",
    "parser.description = 'Use this command parser for training or testing the anomaly detector'\n",
    "parser.add_argument('-m', '--mode', help='train or test mode', choices=['train', 'test'])\n",
    "parser.add_argument('-n', '--normal-data-dir', help='normal I/Q recording directory (for train mode)')\n",
    "parser.add_argument('-a', '--anomaly-data-dir', help='anomaly I/Q recording directory (for test mode)')\n",
    "parser.add_argument('-s', '--weights-save-path', help='path for trained weights (for train mode)')\n",
    "parser.add_argument('-l', '--weights-load-path', help='path for loading weights (for test mode)')\n",
    "\n",
    "namespace = parser.parse_args(sys.argv)\n",
    "if (not namespace.normal_data_dir and namespace.mode == 'train'):\n",
    "    parser.error('the -n arg must be present when mode is train')\n",
    "if (not namespace.weights_save_path and namespace.mode == 'train'):\n",
    "    parser.error('the -s arg must be present when mode is train')\n",
    "\n",
    "if (not namespace.anomaly_data_dir and namespace.mode == 'test'):\n",
    "    parser.error('the -a arg must be present when mode is test')\n",
    "if (not namespace.weights_load_path and namespace.mode == 'test'):\n",
    "    parser.error('the -l arg must be present when mode is test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf=get_config()\n",
    "gpus = conf['gpus']\n",
    "seq_input_length = conf['learning']['rnn']['seq_input_length']\n",
    "seq_output_length = conf['learning']['rnn']['seq_output_length']\n",
    "output_padding = conf['learning']['rnn']['output_padding']\n",
    "input_padding = conf['learning']['rnn']['input_padding']\n",
    "lr=conf['learning']['lr']\n",
    "\n",
    "\n",
    "normal_data_dir = namespace.normal_data_dir\n",
    "anomaly_data_dir = namespace.anomaly_data_dir\n",
    "seq_pad_length = seq_input_length + seq_output_length\n",
    "use_padding = seq_input_length != seq_output_length\n",
    "train = namespace.mode == 'train'\n",
    "opt = Adam(lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if normal_data_dir:\n",
    "    assert len(normal_data_dir) != 0\n",
    "if anomaly_data_dir:\n",
    "    assert len(anomaly_data_dir) != 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading,whitening,scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to:model/lstm/train_scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "#loading,whitening,scaling\n",
    "if train:\n",
    "    train_data = load_train_data(normal_data_dir)\n",
    "else:\n",
    "    test_data = load_test_data(anomaly_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2095104, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.16666669, -0.14285713,  0.4761905 , ...,  0.3214286 ,\n",
       "       -0.09523809,  0.26190478], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:,1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "samples2complex(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the output sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if train:\n",
    "    train_data = series_to_supervised(train_data,n_in=seq_input_length,n_out=seq_output_length)\n",
    "    if anomaly_data_dir:\n",
    "        test_data = series_to_supervised(test_data,n_in=seq_input_length,n_out=seq_output_length)\n",
    "else:\n",
    "    test_data = series_to_supervised(test_data,n_in=seq_input_length,n_out=seq_output_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trim the data to fit the sequence length (SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if train:\n",
    "    train_data = trim_by_seq_length(train_data,seq_input_length)\n",
    "    if anomaly_data_dir:\n",
    "        test_data = trim_by_seq_length(test_data,seq_input_length) \n",
    "else:\n",
    "    test_data = trim_by_seq_length(test_data,seq_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if train:\n",
    "    (X_train,Y_train) = get_X_and_Y_columns(train_data)\n",
    "    if anomaly_data_dir:\n",
    "        (X_test,Y_test) = get_X_and_Y_columns(test_data)\n",
    "else:\n",
    "    (X_test,Y_test) = get_X_and_Y_columns(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if train:\n",
    "    X_train = reshape_to_seq(X_train,seq_input_length)\n",
    "    Y_train = reshape_to_seq(Y_train,seq_output_length)\n",
    "    if anomaly_data_dir:\n",
    "        X_test = reshape_to_seq(X_test,seq_input_length)\n",
    "        Y_test = reshape_to_seq(Y_test,seq_output_length)\n",
    "else:\n",
    "    X_test = reshape_to_seq(X_test,seq_input_length)\n",
    "    Y_test = reshape_to_seq(Y_test,seq_output_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pad input/output sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if use_padding:\n",
    "    if train:\n",
    "        X_train = pad_sequences(X_train,maxlen=seq_pad_length,dtype='float64',padding=input_padding)\n",
    "        Y_train = pad_sequences(Y_train,maxlen=seq_pad_length,dtype='float64',padding=output_padding)\n",
    "        \n",
    "        if anomaly_data_dir:\n",
    "            X_test = pad_sequences(X_test,maxlen=seq_pad_length,dtype='float64',padding=input_padding)\n",
    "            Y_test = pad_sequences(Y_test,maxlen=seq_pad_length,dtype='float64',padding=output_padding)\n",
    "    else:\n",
    "        X_test = pad_sequences(X_test,maxlen=seq_pad_length,dtype='float64',padding=input_padding)\n",
    "        Y_test = pad_sequences(Y_test,maxlen=seq_pad_length,dtype='float64',padding=output_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if train:\n",
    "    inp_shape=X_train.shape\n",
    "else:\n",
    "    inp_shape=X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and loss definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vannila_rnn_model(loss_fn):\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(seq_pad_length, input_shape=(inp_shape[1], inp_shape[2]), return_sequences=True, name='rnn1'))\n",
    "    #     model.add(Dropout(0.5))\n",
    "    model.add(SimpleRNN(seq_pad_length, return_sequences=True, name='rnn2'))\n",
    "    #     model.add(Dropout(0.5))\n",
    "    if use_padding:\n",
    "        model.add(TimeDistributed(Dense(units=2 * seq_pad_length, activation='relu'), name='dense1'))\n",
    "        #         model.add(Dropout(0.5))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(units=2 * seq_pad_length, activation='linear', name='dense2'))\n",
    "        model.add(Reshape((seq_pad_length, 2,)))\n",
    "    else:\n",
    "        model.add(TimeDistributed(Dense(units=2 * seq_output_length, activation='relu'), name='dense1'))\n",
    "        #         model.add(Dropout(0.5))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(units=2 * seq_output_length, activation='linear', name='dense2'))\n",
    "        model.add(Reshape((seq_output_length, 2,)))\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_lstm_model(loss_fn):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(seq_pad_length,input_shape = (inp_shape[1], inp_shape[2]),return_sequences=True,name='lstm1'))\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(LSTM(seq_pad_length,return_sequences=True,name='lstm2'))\n",
    "#     model.add(Dropout(0.5))\n",
    "    if use_padding:\n",
    "        model.add(TimeDistributed(Dense(units=3*seq_pad_length,activation='relu'),name='dense1'))\n",
    "#         model.add(Dropout(0.5))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(units=2*seq_pad_length,activation='linear',name='dense2'))\n",
    "        model.add(Reshape((seq_pad_length,2,)))\n",
    "    else:\n",
    "        model.add(TimeDistributed(Dense(units=2*seq_output_length,activation='relu'),name='dense1'))\n",
    "#         model.add(Dropout(0.5))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(units=2*seq_output_length,activation='linear',name='dense2'))\n",
    "        model.add(Reshape((seq_output_length,2,)))\n",
    "    return model\n",
    "\n",
    "def get_lstm_dense_model(loss_fn):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(seq_input_length,input_shape = (inp_shape[1], inp_shape[2]),return_sequences=True,name='lstm1'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(seq_input_length,return_sequences=True,name='lstm2'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(units=seq_pad_length,activation='relu'),name='dense1'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(units=12,activation='tanh'),name='dense2'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=seq_output_length*2, name='dense3', activation='sigmoid'))\n",
    "    model.add(Reshape((seq_output_length,2,)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if use_padding:\n",
    "    loss_fn = get_clipped_loss()\n",
    "else:\n",
    "    loss_fn = 'mse'\n",
    "\n",
    "if gpus <= 1:\n",
    "    model = get_vannila_rnn_model(loss_fn)\n",
    "    model.summary()\n",
    "    model.compile(optimizer=opt, loss=loss_fn)\n",
    "else:\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        model = get_vannila_rnn_model(loss_fn)\n",
    "        model.summary()\n",
    "    model_multi = multi_gpu_model(model, gpus=gpus)\n",
    "    model_multi.compile(loss=loss_fn,\n",
    "                             optimizer=opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if train:\n",
    "    weights_save_path = namespace.weights_save_path\n",
    "    (X_train, Y_train, X_val, Y_val) = split_train_validation(X_train, Y_train)\n",
    "    if gpus <= 1:\n",
    "        train_model(model, X_train, Y_train, X_val, Y_val)\n",
    "    else:\n",
    "        train_model(model_multi, X_train, Y_train, X_val, Y_val)\n",
    "    model.save(weights_save_path)\n",
    "else:\n",
    "    weights_load_path = namespace.weights_load_path\n",
    "    model.load_weights(weights_load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if train:\n",
    "    if gpus <= 1:\n",
    "        train_errors = predict_error_vectors(X_train, Y_train, model)\n",
    "        val_errors = predict_error_vectors(X_val, Y_val, model)\n",
    "    else:\n",
    "        train_errors = predict_error_vectors(X_train, Y_train, model_multi)\n",
    "        val_errors = predict_error_vectors(X_val, Y_val, model_multi)\n",
    "\n",
    "    train_errors = reshape_errors(train_errors)\n",
    "    val_errors = reshape_errors(val_errors)\n",
    "\n",
    "else:\n",
    "    if gpus <= 1:\n",
    "        test_errors = predict_error_vectors(X_test, Y_test, model)\n",
    "    else:\n",
    "        test_errors = predict_error_vectors(X_test, Y_test, model_multi)\n",
    "    test_errors = reshape_errors(test_errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_scaler_path = 'model/lstm/error_scaler.pkl'\n",
    "train_errors_path = 'model/lstm/train_errors.pkl'\n",
    "val_errors_path = 'model/lstm/val_errors.pkl'\n",
    "if train:\n",
    "    (scaled_train_errors, error_scaler) = scale_train_vectors(train_errors, error_scaler_path)\n",
    "    persist_object(scaled_train_errors, train_errors_path)\n",
    "\n",
    "    scaled_val_errors = error_scaler.transform(val_errors)\n",
    "    persist_object(scaled_val_errors, val_errors_path)\n",
    "\n",
    "    if anomaly_data_dir:\n",
    "        scaled_test_errors = error_scaler.transform(test_errors)\n",
    "else:\n",
    "    scaled_train_errors = load_object(train_errors_path)\n",
    "    scaled_test_errors = scale_test_vectors(test_errors, error_scaler_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error density estimation - GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gmm_save_path = 'model/lstm/gmm.pkl'\n",
    "if train:\n",
    "    gmm = train_gmm(gmm_save_path,scaled_train_errors)\n",
    "else:\n",
    "    gmm=load_object(gmm_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores_path = 'model/lstm/train_scores.pkl'\n",
    "val_scores_path = 'model/lstm/val_scores.pkl'\n",
    "if train:\n",
    "    train_scores = (gmm.score_samples(scaled_train_errors))\n",
    "    persist_object(train_scores, train_scores_path)\n",
    "\n",
    "    val_scores = (gmm.score_samples(scaled_val_errors))\n",
    "    persist_object(val_scores, val_scores_path)\n",
    "    if anomaly_data_dir:\n",
    "        test_scores = (gmm.score_samples(scaled_test_errors))\n",
    "else:\n",
    "    test_scores = (gmm.score_samples(scaled_test_errors))\n",
    "    try:\n",
    "        train_scores = load_object(train_scores_path)\n",
    "    except:\n",
    "        raise Exception('No train scores are found, please train to obtain them')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection phase - EMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset-wise\n",
    "if not train:\n",
    "    # for now, just return the EMD between the train and test scores\n",
    "    emd_dists=compute_emd_distributions(train_scores,test_scores)\n",
    "    print(\"Overall distributions EMD:\", emd_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_emds_path = 'model/lstm/val_emds.pkl'\n",
    "if train:\n",
    "    val_emds = compute_emd_split_samples(val_scores, train_scores)\n",
    "    persist_object(val_emds, val_emds_path)\n",
    "\n",
    "    if anomaly_data_dir:\n",
    "        test_emds = compute_emd_split_samples(test_scores, train_scores)\n",
    "else:\n",
    "    val_emds = load_object(val_emds_path)\n",
    "    test_emds = compute_emd_split_samples(test_scores, train_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# EMD detection experiement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not train:\n",
    "    detect_anomalies_median(val_emds,test_emds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "num_clusters = conf['learning']['num_clusters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = pyplot.subplots(nrows=1, ncols=2,sharey=True , figsize=(20,8))\n",
    "ax0 , ax1 = ax.flatten()\n",
    "if train:\n",
    "    ax0.hist(gmm.predict(scaled_train_errors),bins=np.arange(num_clusters))\n",
    "    if anomaly_data_dir:\n",
    "        ax1.hist(gmm.predict(scaled_test_errors),bins=np.arange(num_clusters))\n",
    "else:\n",
    "    ax1.hist(gmm.predict(scaled_test_errors),bins=np.arange(num_clusters))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utilities.learning import predict_one_sample\n",
    "fig, axes = pyplot.subplots(nrows=1, ncols=3 , figsize=(15,5))\n",
    "ax0,ax1,ax2 = axes.flatten()\n",
    "\n",
    "sample_index=6436\n",
    "\n",
    "pred = predict_one_sample(X_test[sample_index],model)\n",
    "\n",
    "ax0.set_title('true')\n",
    "ax0.plot(Y_test[sample_index][-seq_output_length:])\n",
    "\n",
    "ax1.set_title('pred')\n",
    "ax1.plot(pred[-seq_output_length:])\n",
    "\n",
    "ax2.set_title('diff')\n",
    "ax2.plot(np.abs(pred[-seq_output_length:] - Y_test[sample_index][-seq_output_length:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, ax = pyplot.subplots(nrows=1, ncols=1,sharex=True)\n",
    "ax.set_title('error likelihood scores' , fontsize=15)\n",
    "# ax.set_xlim(-1000,1000)\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set_fontsize(15)\n",
    "        \n",
    "if train:\n",
    "    width = np.min(np.diff(train_bins))/3 #voodo shit\n",
    "    pyplot.bar(train_bins[:-1] , train_hist , width=width , color='b')\n",
    "    if anomaly_data_dir:\n",
    "        pyplot.bar(test_bins[:-1] , test_hist , width=width , color='r')\n",
    "\n",
    "else:\n",
    "    width = np.min(np.diff(test_bins))/3 #voodo shit\n",
    "    pyplot.bar(test_bins[:-1] , test_hist , width=width , color='r')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_train_errors = np.sum(scaled_train_errors,axis=1)\n",
    "sum_test_errors = np.sum(scaled_test_errors,axis=1)\n",
    "(n1, bins, _) = pyplot.hist(sum_train_errors,bins=100,color='b',alpha=0.5)\n",
    "(n2, _ , _) = pyplot.hist(sum_test_errors,bins=100,color='r',alpha=0.5)\n",
    "# (n3, _ , _) = pyplot.hist(sum_test_errors_anomaly,bins=[x/100 for x in range(100)],color='y',alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(n1, bins, _) = pyplot.hist(train_scores,bins=200,color='b',alpha=0.5)\n",
    "(n2, _ , _) = pyplot.hist(test_scores,bins=200,color='r',alpha=0.5)\n",
    "# (n3, _ , _) = pyplot.hist(test_scores_falseneg,bins=list(range(-200,100, 5)),color='y',alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train[:,:,0]+1j*X_train[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def samples2complex(samples):\n",
    "    return samples[:,:,0]+1j*samples[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-3ce4c3a397f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msamples2complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "samples2complex(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
