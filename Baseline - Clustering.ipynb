{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "from skimage.util import view_as_windows\n",
    "from utilities.config_handler import get_config\n",
    "from utilities.learning import split_train_validation, train_model, predict_ae_error_vectors\n",
    "from utilities.detection import detect_reconstruction_anomalies_median,plot_spectogram_anomalies\n",
    "from utilities.preprocessing import  add_noise,load_fft_test_data ,load_fft_train_data,  reshape_to_blocks,persist_object\\\n",
    "    ,load_object,persist_val_stat, load_val_stat ,get_xhdr_sample_rate\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import mixture\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin_min\n",
    "import pandas as pd\n",
    "from utilities.visualization import plot_spectogram\n",
    "from scipy.stats import entropy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_records = ['CELL_NORM_2', 'CELL_NORM_3', 'CELL_NORM_4']\n",
    "anomal_records = ['CELL_CW_-20MHz_0dB', 'CELL_CW_-20MHz_10dB', 'CELL_SWP_18MHz_50us_0dB', \\\n",
    "          'CELL_SWP_18MHz_50us_10dB', 'CELL_SWP_18MHz_100us_0dB', 'CELL_SWP_18MHz_100us_10dB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.prog = 'Spectrum Anomaly Detection'\n",
    "parser.description = 'Use this command parser for training or testing the anomaly detector'\n",
    "parser.add_argument('-m', '--mode', help='train or test mode', choices=['train', 'test'])\n",
    "parser.add_argument('-d', '--data-dir', help='I/Q recording directory')\n",
    "parser.add_argument('-w', '--weights-path', help='path for trained weights')\n",
    "\n",
    "\n",
    "sys.argv = \"-m train -d iq_data/CELL/normal/CELL_NORM_2 -w model/baseline_clustering\".split()\n",
    "# sys.argv = \"-m test -d iq_data/CELL/normal/CELL_NORM_4 -w model/baseline_clustering\".split()\n",
    "namespace = parser.parse_args(sys.argv)\n",
    "if not namespace.data_dir and namespace.mode == 'train':\n",
    "    parser.error('the -d arg must be present when mode is train')\n",
    "if not namespace.weights_path and namespace.mode == 'train':\n",
    "    parser.error('the -w arg must be present when mode is train')\n",
    "\n",
    "if not namespace.data_dir and namespace.mode == 'test':\n",
    "    parser.error('the -d arg must be present when mode is test')\n",
    "if not namespace.weights_path and namespace.mode == 'test':\n",
    "    parser.error('the -w arg must be present when mode is test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf=get_config()\n",
    "gpus = conf['gpus']\n",
    "lr=conf['learning']['ae']['lr']\n",
    "validation_split = conf['learning']['ae']['validation_split']\n",
    "train_params = conf['learning']['ae']\n",
    "rbw_set = conf['preprocessing']['ae']['rbw_set']\n",
    "feature_names = conf['preprocessing']['ae']['feature_names']\n",
    "train = namespace.mode == 'train'\n",
    "\n",
    "data_dir = namespace.data_dir\n",
    "\n",
    "atom_height = 1\n",
    "fraction = 0.2\n",
    "num_clusters = 100\n",
    "f_s = get_xhdr_sample_rate(data_dir)\n",
    "test_window_time = 10e-6\n",
    "cv_types = ['diag']\n",
    "\n",
    "assert len(data_dir) != 0\n",
    "dataset_name = str.split(data_dir, '/')[1]\n",
    "recording_name = str.split(data_dir,'/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_spectogram_to_atoms(spectogram,atom_height,stride=1):\n",
    "    window_shape = (atom_height , spectogram.shape[1])\n",
    "    return view_as_windows(spectogram,window_shape,step=stride).reshape(-1,*window_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gmm(data,num_clusters,cv_types):\n",
    "    lowest_bic = np.infty\n",
    "    bic = []\n",
    "    best_component=''\n",
    "    best_cv=''\n",
    "    best_gmm={}\n",
    "    n_components_range = np.arange(2, num_clusters,20) # specifying maximum number of clusters\n",
    "    for cv_type in cv_types:\n",
    "        for n_components in n_components_range:\n",
    "            # Fit a mixture of Gaussians with EM\n",
    "            gmm = mixture.GaussianMixture(n_components=num_clusters, covariance_type=cv_type,verbose=2)\n",
    "            gmm.fit(data)\n",
    "            bic.append(gmm.bic(data))\n",
    "            if bic[-1] < lowest_bic:\n",
    "                lowest_bic = bic[-1]\n",
    "                best_component = n_components\n",
    "                best_cv = cv_type\n",
    "                best_gmm = gmm\n",
    "                print(best_cv)\n",
    "                print(lowest_bic)\n",
    "    print(\"best n_component {}\".format(best_component))\n",
    "    print(\"best gmm type {}\".format(best_cv))\n",
    "    return best_gmm\n",
    "\n",
    "def fit_kmeans(data,num_clusters):\n",
    "    k_means = KMeans(init='k-means++', n_clusters=num_clusters,n_jobs=-1,precompute_distances=True,n_init=3)\n",
    "    k_means.fit(data)\n",
    "    return k_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_spectogram_by_fraction(spectogram,frac):\n",
    "    return spectogram[:int(frac*len(spectogram))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consecutive(data, stepsize=1):\n",
    "    return [list(part) for part in np.split(data, np.where(np.diff(data) != stepsize)[0]+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to:model/baseline_clustering/CELL_125000.0/k_means_k=100_a=1.pkl\n",
      "saving to:model/baseline_clustering/CELL_125000.0/train_clusters_distances_k=100_a=1.pkl\n",
      "saving to:model/baseline_clustering/CELL_125000.0/max_cluster_distance_k=100_a=1.pkl\n"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    for rbw in rbw_set:\n",
    "        weights_dir = \"_\".join((dataset_name, str(rbw)))\n",
    "        weights_path = os.path.join(namespace.weights_path, weights_dir)\n",
    "        _,_,fft_train = load_fft_train_data(data_dir, rbw, weights_path)\n",
    "        fft_train = split_spectogram_by_fraction(fft_train,fraction)\n",
    "        atom_fft_train = split_spectogram_to_atoms(fft_train,atom_height)\n",
    "        flatten_atom_fft_train = atom_fft_train.reshape(len(atom_fft_train),-1)\n",
    "        \n",
    "        k_means = fit_kmeans(flatten_atom_fft_train,num_clusters)\n",
    "        (min_clusters_train,min_distances_train) = pairwise_distances_argmin_min(\\\n",
    "                                                    flatten_atom_fft_train,k_means.cluster_centers_,metric='euclidean')\n",
    "        \n",
    "        train_clusters_distances_df = pd.DataFrame({'cluster':min_clusters_train , 'distance':min_distances_train})\n",
    "        max_cluster_distance_train = train_clusters_distances_df.groupby('cluster').max()\n",
    "        persist_object(k_means,os.path.join(weights_path,'k_means_k={}_a={}.pkl'.format(num_clusters,atom_height)))\n",
    "        persist_object(train_clusters_distances_df,os.path.join(weights_path,\\\n",
    "                                            'train_clusters_distances_k={}_a={}.pkl'.format(num_clusters,atom_height)))\n",
    "        persist_object(max_cluster_distance_train , os.path.join(weights_path,\\\n",
    "                                                 'max_cluster_distance_k={}_a={}.pkl'.format(num_clusters,atom_height)))\n",
    "        \n",
    "else:\n",
    "    \n",
    "    for rbw in rbw_set:\n",
    "        weights_dir = \"_\".join((dataset_name, str(rbw)))\n",
    "        weights_path = os.path.join(namespace.weights_path, weights_dir)\n",
    "        freqs_test, time_test, fft_test = load_fft_test_data(data_dir, rbw, weights_path)\n",
    "        \n",
    "        fft_test = split_spectogram_by_fraction(fft_test,fraction)\n",
    "        atom_fft_test = split_spectogram_to_atoms(fft_test,atom_height)\n",
    "        flatten_atom_fft_test = atom_fft_test.reshape(len(atom_fft_test),-1)\n",
    "        k_means = load_object(os.path.join(weights_path,'k_means_k={}_a={}.pkl'.format(num_clusters,atom_height)))\n",
    "        train_clusters_distances_df = load_object(os.path.join(weights_path,'train_clusters_distances_k={}_a={}.pkl'\\\n",
    "                                                       .format(num_clusters,atom_height)))\n",
    "        \n",
    "        max_cluster_distance_train = load_object(os.path.join(weights_path,\\\n",
    "                                                 'max_cluster_distance_k={}_a={}.pkl'.format(num_clusters,atom_height)))\n",
    "        \n",
    "        window_shape = (atom_height , atom_fft_test.shape[1] // atom_height)\n",
    "        score_spectogram = np.ones_like(atom_fft_test)\n",
    "        (min_clusters_test,min_distances_test) = pairwise_distances_argmin_min(flatten_atom_fft_test,k_means.cluster_centers_,\\\n",
    "                                                               metric='euclidean')\n",
    "        \n",
    "        test_clusters_distances_df = pd.DataFrame({'cluster':min_clusters_test , 'distance':min_distances_test})\n",
    "        test_clusters_distances_df['anomaly'] = test_clusters_distances_df\\\n",
    "                                .apply(lambda x : x.distance > max_cluster_distance_train.iloc[int(x.cluster)] , axis=1)\n",
    "        \n",
    "        anomalies_indices = np.argwhere(test_clusters_distances_df.anomaly).squeeze().tolist()\n",
    "        \n",
    "        f, ax = plt.subplots(figsize=(10,10))\n",
    "        percent_anomalies = len(anomalies_indices) / len(test_clusters_distances_df) * 100\n",
    "        ax.plot(np.sort(min_distances_test))\n",
    "#         for index, row in max_cluster_distance_train.iterrows():\n",
    "#             ax.axhline(row.distance,color='r',linewidth=0.1)\n",
    "            \n",
    "        ax.set_title('Test minimum distances, num clusters {0:d} ,atom height {1}, num anomolous atoms {2:.2f}%'.\\\n",
    "                      format(num_clusters,atom_height,percent_anomalies))\n",
    "        \n",
    "        distances_plot_path = os.path.join(namespace.data_dir ,'cluster_distances_k={}_a={}_{}.png'.\\\n",
    "                                           format(num_clusters,atom_height,recording_name))\n",
    "        plt.savefig(distances_plot_path)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(20,20))\n",
    "        anomalies_blocks = consecutive(anomalies_indices)\n",
    "        ax.imshow(fft_test,aspect='auto', origin='upper')\n",
    "        if len(anomalies_indices) > 0:\n",
    "            anomalies_plot_path = os.path.join(namespace.data_dir ,'cluster_anomalies_k={}_a={}_{}.png'.\\\n",
    "                                           format(num_clusters,atom_height,recording_name))\n",
    "            for block in anomalies_blocks:\n",
    "                x_cord = 0\n",
    "                y_cord = block[0]\n",
    "                block_height , block_width = len(block) + 1,fft_test.shape[1]\n",
    "                rect = patches.Rectangle((x_cord,y_cord),block_width-1,block_height,edgecolor='r',\\\n",
    "                                         facecolor='r',fill=True,alpha=0.5,linewidth=0.1,rasterized=True)\n",
    "                ax.add_patch(rect)\n",
    "        \n",
    "        plt.savefig(anomalies_plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectrum visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
