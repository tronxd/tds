{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Input\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Model\n",
    "from keras.layers import Reshape\n",
    "from keras import regularizers\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from functools import partial\n",
    "from keras import backend as K\n",
    "from keras.losses import mean_absolute_error,mean_squared_error\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import sys\n",
    "import argparse\n",
    "from keras.optimizers import Adam\n",
    "from skimage.util import view_as_blocks\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utilities.config_handler import get_config\n",
    "from utilities.learning import   split_train_validation,  train_model, predict_ae_error_vectors\n",
    "from utilities.preprocessing import  add_noise,load_fft_test_data ,load_fft_train_data, reshape_to_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.argv = '-m train -n iq_data/gps_new/norm --weights-save-path model/ae/ae_model.hdf5'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.prog = 'Spectrum Anomaly Detection'\n",
    "parser.description = 'Use this command parser for training or testing the anomaly detector'\n",
    "parser.add_argument('-m', '--mode', help='train or test mode', choices=['train', 'test'])\n",
    "parser.add_argument('-n', '--normal-data-dir', help='normal I/Q recording directory (for train mode)')\n",
    "parser.add_argument('-a', '--anomaly-data-dir', help='anomaly I/Q recording directory (for test mode)')\n",
    "parser.add_argument('-s', '--weights-save-path', help='path for trained weights (for train mode)')\n",
    "parser.add_argument('-l', '--weights-load-path', help='path for loading weights (for test mode)')\n",
    "\n",
    "namespace = parser.parse_args(sys.argv)\n",
    "if (not namespace.normal_data_dir and namespace.mode == 'train'):\n",
    "    parser.error('the -n arg must be present when mode is train')\n",
    "if (not namespace.weights_save_path and namespace.mode == 'train'):\n",
    "    parser.error('the -s arg must be present when mode is train')\n",
    "\n",
    "if (not namespace.anomaly_data_dir and namespace.mode == 'test'):\n",
    "    parser.error('the -a arg must be present when mode is test')\n",
    "if (not namespace.weights_load_path and namespace.mode == 'test'):\n",
    "    parser.error('the -l arg must be present when mode is test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf=get_config()\n",
    "gpus = conf['gpus']\n",
    "lr=conf['learning']['lr']\n",
    "use_noise=conf['preprocessing']['ae']['use_noise']\n",
    "feature_names = conf['preprocessing']['ae']['feature_names']\n",
    "rbw_l = conf['preprocessing']['ae']['rbw_l']\n",
    "rbw_s = conf['preprocessing']['ae']['rbw_s']\n",
    "block_size = conf['preprocessing']['ae']['block_size']\n",
    "\n",
    "train_scaler_ae_large_path = conf['preprocessing']['ae']['train_scaler_path_large']\n",
    "train_scaler_ae_small_path =conf['preprocessing']['ae']['train_scaler_path_small']\n",
    "train_zca_ae_path_large = conf['preprocessing']['ae']['train_zca_path_large']\n",
    "train_zca_ae_path_small = conf['preprocessing']['ae']['train_zca_path_small']\n",
    "\n",
    "normal_data_dir = namespace.normal_data_dir\n",
    "anomaly_data_dir = namespace.anomaly_data_dir\n",
    "train = namespace.mode == 'train'\n",
    "opt = Adam(lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if normal_data_dir:\n",
    "    assert len(normal_data_dir) != 0\n",
    "if anomaly_data_dir:\n",
    "    assert len(anomaly_data_dir) != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_conv_autoencoder_model(input_shape):\n",
    "    num_features=input_shape[0]\n",
    "    block_length = input_shape[1]\n",
    "    inputs = Input(shape=input_shape,name='input')\n",
    "    conv1 = Conv2D(1, (num_features , int(block_length / 2) - 4 ), activation='linear', padding='same')(inputs)\n",
    "\n",
    "    conv1_flat = Flatten()(conv1)\n",
    "#      h1 = Dense((int(block_length / 2)), activation=K.hard_sigmoid ,\n",
    "#                 activity_regularizer=regularizers.l1(10e-5) ,\n",
    "#                 kernel_regularizer=regularizers.l2(0.5))(conv1_flat)\n",
    "\n",
    "    h1=Dense(int(block_length) ,activation=K.sigmoid , activity_regularizer=regularizers.l1(0) , name='hidden1')(conv1_flat)\n",
    "#     h2 = Dense(num_features * block_length , activation=K.hard_sigmoid ,\n",
    "#                 activity_regularizer=regularizers.l1(10e-5) ,\n",
    "#                 kernel_regularizer=regularizers.l2(0.1))(h1)\n",
    "\n",
    "    h2=Dense(block_length*num_features , activation=K.sigmoid, activity_regularizer=regularizers.l1(0) , name='hidden2')(h1)\n",
    "    h2_reshape = Reshape(input_shape)(h2)\n",
    "    outputs = Conv2D(1, ((int(block_length / 2) - 4), 1), activation='linear', padding='same')(h2_reshape)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading,whitening,scaling,fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    weights_save_path = namespace.weights_save_path\n",
    "    fft_train_large = load_fft_train_data(normal_data_dir , rbw_l , train_scaler_ae_large_path,train_zca_ae_path_large)\n",
    "    fft_train_small = load_fft_train_data(normal_data_dir,rbw_s , train_scaler_ae_small_path,train_zca_ae_path_small)\n",
    "    #TODO infer block size\n",
    "    X_train_large = reshape_to_blocks(fft_train_large,block_size)\n",
    "    X_train_small = reshape_to_blocks(fft_train_small,block_size)\n",
    "    (X_train_large, _, X_val_large, _) = split_train_validation(X_train_large, X_train_large)\n",
    "    (X_train_small, _, X_val_small, _) = split_train_validation(X_train_small, X_train_small)\n",
    "else:\n",
    "    #TODO also make it generic\n",
    "    weights_load_path = namespace.weights_load_path\n",
    "    fft_test = load_fft_test_data(anomaly_data_dir , rbw_l ,train_scaler_ae_large_path , train_zca_ae_path_large)\n",
    "    X_test = reshape_to_blocks(fft_test,block_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(fft_blocks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50,50))\n",
    "plt.imshow(fft_train_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,40))\n",
    "plt.imshow(fft_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    conv_model_large = get_conv_autoencoder_model(X_train_large.shape[1:])\n",
    "    conv_model_small = get_conv_autoencoder_model(X_train_small.shape[1:])\n",
    "    if use_noise:\n",
    "        X_train_large_noisy = add_noise(X_train_large)\n",
    "        X_train_small_noisy = add_noise(X_train_small)\n",
    "        train_model(conv_model_large,X_train_large_noisy,X_train_large , X_val=X_val_large, Y_val=X_val_large)\n",
    "    else:\n",
    "        train_model(conv_model_large,X_train_large,X_train_large,X_val=X_val_large, Y_val=X_val_large)\n",
    "\n",
    "    conv_model_large.save_weights(weights_save_path)\n",
    "    conv_model_small.save_weights(weights_save_path)\n",
    "    train_errors = predict_ae_error_vectors(X_train_large, X_train_large, conv_model_large)\n",
    "    \n",
    "else:\n",
    "    #TODO also make it generic\n",
    "    weights_load_path = namespace.weights_load_path\n",
    "    conv_model = get_conv_autoencoder_model(X_test.shape[1:])\n",
    "    conv_model.load_weights(weights_load_path)\n",
    "    test_errors = predict_ae_error_vectors(X_test, X_test, conv_model)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_prediction_sample(x,ax,title=None):\n",
    "    #X.shape = (1, 2, 128, 1)\n",
    "    x = np.squeeze(x)\n",
    "    I = x[0]\n",
    "    Q = x[1]\n",
    "    ax.plot(I)\n",
    "    ax.plot(Q)\n",
    "    if title:\n",
    "        ax.set_title(title,fontsize=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_index=120\n",
    "sample = X_test[sample_index]\n",
    "pred = model.predict(np.expand_dims(sample,0))\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2,figsize=(40,15),sharey=True)\n",
    "for label in (ax1.get_xticklabels() + ax1.get_yticklabels()):\n",
    "    label.set_fontsize(25)\n",
    "    \n",
    "for label in (ax2.get_xticklabels() + ax2.get_yticklabels()):\n",
    "    label.set_fontsize(25)\n",
    "    \n",
    "plot_prediction_sample(sample[:16],ax1,'real signal')\n",
    "plot_prediction_sample(pred[:16],ax2,'reconstruction')\n",
    "\n",
    "print(np.sqrt(np.sum(np.square(pred - sample)))/len(pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(data, batch_size):\n",
    "    l = len(data)\n",
    "    for ndx in range(0, l, batch_size):\n",
    "        yield data[ndx:min(ndx + batch_size, l)]\n",
    "        \n",
    "\n",
    "def predict_error_vectors(X,Y,model,batch_size = batch_size):  \n",
    "    i=0\n",
    "    errors = np.empty((X_train.shape[0]))\n",
    "    for (batch_X,batch_Y) in zip(get_batch(X,batch_size),get_batch(Y,batch_size)):\n",
    "        Y_pred = model.predict_on_batch(batch_X)\n",
    "        batch_error = compute_batch_error(batch_Y,Y_pred)\n",
    "        errors[i*batch_size:(i+1)*batch_size] = batch_error\n",
    "        i=i+1\n",
    "        if i%50 == 0:\n",
    "            print('Prediction batch {:d} / {:d}'.format(i,int(len(X)/(batch_size))))\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_errors = predict_error_vectors(X_train,X_train,model)\n",
    "test_errors = predict_error_vectors(X_test,X_test,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(train_errors,bins=100)\n",
    "plt.hist(test_errors,bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
