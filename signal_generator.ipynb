{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import struct\n",
    "import xml.etree.ElementTree\n",
    "import os\n",
    "\n",
    "def xdat2array(xdat_path,little_endian):\n",
    "    with open(xdat_path,'rb') as f:\n",
    "        b = f.read()\n",
    "        num_shorts = int(len(b) / 2)\n",
    "        if little_endian:\n",
    "            xdat = struct.unpack(\"<{}h\".format(num_shorts),b)\n",
    "        else:\n",
    "            xdat = struct.unpack(\">{}h\".format(num_shorts),b)\n",
    "        xdat_array = np.array(xdat,dtype='float32')\n",
    "        xdat_array = xdat_array.reshape((int(len(xdat_array)/2),2))\n",
    "        return xdat_array\n",
    "\n",
    "#calculates the overall number of samples (shorts) of all xdat files in a given directory (for efficient allocation)\n",
    "def calc_num_xdat_samples(data_dir):\n",
    "    num_samples = 0\n",
    "    xdat_data_files =[os.path.join(data_dir,file) for file in os.listdir(data_dir) if file.endswith('.xdat')]\n",
    "    for file in xdat_data_files:\n",
    "        num_samples = num_samples + int(os.path.getsize(file)/2) #divide by 2 because short = 2 bytes\n",
    "    return num_samples\n",
    "\n",
    "def get_xhdr_scale_factor(xhdr_path):\n",
    "    e=xml.etree.ElementTree.parse(xhdr_path)\n",
    "    return float(e.find('captures').find('capture').get('acq_scale_factor'))\n",
    "\n",
    "#assuming sampling rate is the same in consecutive xdat recordings\n",
    "def get_xhdr_sample_rate(data_dir):\n",
    "    xhdr_files = [os.path.join(data_dir,file) for file in os.listdir(data_dir) if file.endswith('.xhdr')]\n",
    "    xhdr_path = xhdr_files[0]\n",
    "    e=xml.etree.ElementTree.parse(xhdr_path)\n",
    "    return int(e.find('captures').find('capture').get('sample_rate'))\n",
    "\n",
    "\n",
    "def get_xhdr_little_endian(xhdr_path):\n",
    "    e=xml.etree.ElementTree.parse(xhdr_path)\n",
    "    return bool(e.find('data_files').find('data').get('little_endian'))\n",
    "\n",
    "\n",
    "def load_xdat_data(data_files,num_samples):\n",
    "    xdat = np.zeros((int(num_samples/2) , 2),dtype='float32')\n",
    "    pos = 0\n",
    "    for file in data_files:\n",
    "        xhdr_path = \"\".join((os.path.splitext(file)[0],'.xhdr'))\n",
    "        scale_factor = get_xhdr_scale_factor(xhdr_path)\n",
    "        little_endian = get_xhdr_little_endian(xhdr_path)\n",
    "        xdat_part = xdat2array(file,little_endian)\n",
    "        xdat_part = scale_factor * 2**16 * xdat_part\n",
    "        curr = pos + len(xdat_part)\n",
    "        xdat[pos : curr,:] = xdat_part\n",
    "        pos = curr\n",
    "    return xdat\n",
    "\n",
    "#Return data,data_type\n",
    "def load_raw_data(data_dir):\n",
    "    xdat_data_files =[os.path.join(data_dir,file) for file in os.listdir(data_dir) if file.endswith('.xdat')]\n",
    "    data = []\n",
    "    if len(xdat_data_files) > 0:\n",
    "        num_samples = calc_num_xdat_samples(data_dir)\n",
    "        data = load_xdat_data(xdat_data_files,num_samples)\n",
    "    return data\n",
    "\n",
    "def samples2complex(samples):\n",
    "    return samples[:,0]+1j*samples[:,1]\n",
    "\n",
    "def complex2power(complex_data):\n",
    "    return 20*np.log10(1000*np.absolute(complex_data))\n",
    "\n",
    "def trim_by_seq_length(data, seq_length):\n",
    "    num_samples = data.shape[0]\n",
    "    trim_num_samples = num_samples - num_samples % seq_length\n",
    "    # trim to the data to fit sequence length\n",
    "    trim_data = data[:trim_num_samples]\n",
    "    return trim_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import max,min,imag,real\n",
    "\n",
    "def get_normal_data(filename):\n",
    "    filename_xdat = filename + \".xdat\"\n",
    "    filename_xhdr = filename + \".xhdr\"\n",
    "    scale = get_xhdr_scale_factor(filename_xhdr)\n",
    "    fs = get_xhdr_sample_rate(data_dir)\n",
    "    data = samples2complex(xdat2array(filename_xdat,True))\n",
    "    ts = 1/fs\n",
    "    \n",
    "    return data,fs,ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cw(x_len, ts, fc):\n",
    "    t = np.arange(0,x_len)*ts\n",
    "    cw = np.exp(1j*2*np.pi*fc*t);\n",
    "def channel(data):\n",
    "    R = np.random.rayleigh(size=data.size)\n",
    "    return data*R.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import butter, lfilter, freqz, kaiserord, firwin\n",
    "\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyq_rate = fs/2\n",
    "    normal_cutoff = cutoff / nyq_rate\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def kaiser_lowpass(cutoff, fs, order=5):\n",
    "    nyq_rate = fs\n",
    "    width = cutoff*1.1/nyq_rate\n",
    "    ripple_db = 60.0\n",
    "    N, beta = kaiserord(ripple_db, width)\n",
    "    taps = firwin(N, cutoff/nyq_rate, window=('kaiser', beta))\n",
    "    return taps, 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "#from numpy.matlib import repmap\n",
    "#from scipy.signal import chirp\n",
    "\n",
    "def sweep(x_len, fs, f0, t1, f1, filtered=True, order=20):\n",
    "    t0 = 0; #sec\n",
    "    fs = 1/ts\n",
    "    b = (f1-f0)/(t1-t0)\n",
    "\n",
    "    sweep = zeros(x_len,dtype=\"complex64\")\n",
    "    t = np.arange(t0,t1,ts);\n",
    "    t_len = len(t)\n",
    "\n",
    "    for k in np.arange(0,x_len//t_len):\n",
    "        del_b=0.5*b*np.random.rand();\n",
    "        sweep[t_len*k:t_len*(k+1)]=np.exp(1j*2*np.pi*(f0+0.5*(b+del_b)*t)*t)\n",
    "\n",
    "    if (sweep.size < x_len):\n",
    "        sweep.append(zeros(data_len-sweep.size))\n",
    "    elif(sweep.size > x_len):\n",
    "        sweep = sweep[1:x_len]\n",
    "        \n",
    "    if(filtered):\n",
    "        return filter_sweep(sweep,fs,f0,f1)\n",
    "    else:\n",
    "        return sweep\n",
    "        \n",
    "def filter_sweep(data,fs,f0,f1,order=20):\n",
    "    f_co = (abs(f0-f1))/2;\n",
    "    filt = butter_lowpass(f_co,fs, order=order)\n",
    "    fc = (f0+f1)/2\n",
    "    t = np.arange(0,data.size)/fs\n",
    "    data *= np.exp(-1j*2*np.pi*fc*t) # move to bb\n",
    "    data = lfilter(filt[0],filt[1],data) # filter\n",
    "    data *= np.exp(1j*2*np.pi*fc*t) # bring back\n",
    "    \n",
    "    return data\n",
    "    # for k in np.arange(0,data_len//t_sw_len):\n",
    "    #     del_b=0.5*b*np.random.rand();\n",
    "    #     sweep[t_sw_len*k:t_sw_len*(k+1)] = chirp(t=t_sw,f0=fstart,f1=fstop,t1=t_stop)\n",
    "\n",
    "    # sweep(1:t_len) = exp(1j*2*pi*(fstart+0.5*b*t).*t);\n",
    "    # sweep=repmat(sweep,1,(floor((length(record_rec))/(length(sweep))))); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from scipy.signal import spectrogram,get_window\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# _,_,spec = spectrogram(x=sweep,fs=fs,window=get_window(\"blackmanharris\",1024),return_onesided=False, mode='complex',nperseg=1024)\n",
    "# spec = complex2power(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#t_sw_len\n",
    "# np.argwhere(np.isnan(spec))\n",
    "# plt.imshow(spec[0:len(spec)//10],aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(np.argwhere(np.isnan(sweep)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def absolute_max(data):\n",
    "    maxi = max((data.real.max(),data.imag.max()))\n",
    "    mini = min((data.real.min(),data.imag.min()))\n",
    "    return max((maxi,abs(mini)))\n",
    "\n",
    "def scale_dist_to_data(data,disturbance):\n",
    "    abs_max_data = absolute_max(data)\n",
    "    abs_max_dist = absolute_max(disturbance)\n",
    "    \n",
    "    disturbance = disturbance/abs_max_dist*(np.iinfo(np.int16).max-abs_max_data-1)\n",
    "    \n",
    "    return disturbance\n",
    "\n",
    "# disturbance = disturbance/10**(disturbance_atten_db/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "from time import time\n",
    "\n",
    "\n",
    "def save_signal(filename,data):\n",
    "    if(absolute_max(data) > np.iinfo(np.int16).max):\n",
    "        print(\"Scaling problem. Please attenuate disturbance\");\n",
    "        exit()\n",
    "\n",
    "    B = np.zeros( (data.size,2) )\n",
    "    B[:,0] = real(data)\n",
    "    B[:,1] = imag(data)\n",
    "    B = np.reshape(B,(-1,)).astype('int16')\n",
    "    \n",
    "    filename_xhdr = filename + \".xhdr\"\n",
    "    new_filename = filename + \"_new_\" + str(int(time()))\n",
    "    new_filename_xdat = new_filename + \".xdat\"\n",
    "    new_filename_xhdr = new_filename + \".xhdr\"\n",
    "    file = open(new_filename_xdat,'wb');\n",
    "    file.write(struct.pack('<{}h'.format(len(B)),*B))\n",
    "    file.close()\n",
    "    copyfile(filename_xhdr,new_filename_xhdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"test\\\\\"\n",
    "filename = \"CELL_NORM\"\n",
    "data,fs,ts = get_normal_data(data_dir + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_start = -5e6\n",
    "f_stop = 5e6\n",
    "sweep_time = 100e-6\n",
    "disturbance = sweep(data.size, ts, f_start, sweep_time, f_stop, filtered=False)\n",
    "disturbance //= 10\n",
    "disturbance = channel(disturbance)\n",
    "disturbance = filter_sweep(disturbance,fs,f_start,f_stop)\n",
    "disturbance = scale_dist_to_data(data=data,disturbance=disturbance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_signal = data + disturbance\n",
    "save_signal(data_dir + filename,total_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-73.73674601302794"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "20*np.log10(3.27217e-12*62853120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
